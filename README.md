# ğŸš€ Databricks & IDC 14 Days AI Challenge

This repository documents my **14-day hands-on learning journey & capston project on Databricks**, focused on building a strong foundation in **data engineering, analytics, and AI workflows** using Apache Spark and the Lakehouse architecture.

The challenge follows a **daily structured format** combining concepts, practice tasks, and real-world use cases.

## ğŸ“Œ Table of Contents

- [About the Challenge](#-about-the-challenge)
- [Topics of learning](#-topics-of-learning)
- [Challenge Structure](#-challenge-structure)
- [Daily Progress Tracker](#-daily-progress-tracker)
- [Tech Stack](#-tech-stack)
- [Learning Objective](#-learning-objective)
- [Acknowledgements](#-acknowledgements)
- [Connect With Me](#-connect-with-me)
  
---

## ğŸ¢ About the Challenge

- ğŸ“Œ Organized by **Indian Data Club**
- ğŸ¤ In collaboration with **Codebasics**
- â­ Sponsored by **Databricks**

This initiative is designed to help learners move from **Databricks fundamentals to AI-powered analytics** through practical, project-based learning.

---

## ğŸ¯ Topics of learning

By completing this 14-day journey, learners will gain a solid foundation in **data engineering, analytics, and AI on Databricks**. Key learning areas include:  

- **Databricks Platform & Lakehouse Architecture** â€“ Understanding the ecosystem and how data flows from raw to analytical layers.  
- **Apache Spark & PySpark** â€“ Performing distributed data processing and transformations on large datasets.  
- **Delta Lake & Data Engineering Patterns** â€“ Implementing ACID transactions, versioning, and structured pipelines.  
- **Medallion Architecture (Bronzeâ€“Silverâ€“Gold)** â€“ Building clean and efficient layers for staged and curated datasets.  
- **Workflows, Jobs & Governance** â€“ Automating pipelines and managing access with Unity Catalog.  
- **SQL Analytics & Performance Optimization** â€“ Running queries efficiently and tuning performance for analytics workloads.  
- **MLflow, AI & Generative AI** â€“ Managing ML experiments, comparing models, and experimenting with AI-driven insights.  

This ensures a **comprehensive understanding of the Databricks ecosystem**, bridging the gap between theory and real-world application.  

---

## ğŸ—ºï¸ Challenge Structure

The challenge is structured into **four phases**, progressing from fundamentals to advanced AI analytics:  

### **PHASE 1: Foundation (Days 1â€“4)**  
Covers Databricks setup, basic concepts, Spark fundamentals, and data transformations. Learners gain hands-on experience with notebooks, RDDs, and DataFrames.  

### **PHASE 2: Data Engineering (Days 5â€“8)**  
Focuses on **Delta Lake, Medallion Architecture**, and job automation. Students learn to design efficient pipelines, implement versioning, and manage governance.  

### **PHASE 3: Advanced Analytics (Days 9â€“11)**  
Teaches **SQL analytics, performance tuning, and statistical preparation**. Learners work on real datasets to create interactive queries and optimized analytics.  

### **PHASE 4: AI & ML (Days 12â€“14)**  
Introduces **MLflow, model comparison, and generative AI techniques**. Learners track experiments, evaluate models, and build AI-driven solutions to extract insights.  

By the end of the challenge, learners can confidently handle **end-to-end Databricks workflows**, from raw data ingestion to AI-powered analytics.

---

## ğŸ§  Daily Progress Tracker

| Day | Topic | Description | LinkedIn Post |
|----|------|-------------|---------------|
| âœ… D0 | Setup & Data Loading (Prerequisites) | Installed Databricks and prepared datasets for the challenge | [LinkedIn Post](https://www.linkedin.com/posts/nimiyakujjwal_databrickswithidc-databricks-learninginpublic-activity-7416131231203053568-g5_O?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB-AfacBUlcUhAKRaQjr5MosRdgTWZAR600) |
| âœ… D1 | Platform Setup & First Steps ğŸš€ | Explored Databricks workspace, notebooks, and first basic operations | [LinkedIn Post](https://www.linkedin.com/posts/nimiyakujjwal_databrickswithidc-databricks-learninginpublic-activity-7416133451340431360-TIRk?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB-AfacBUlcUhAKRaQjr5MosRdgTWZAR600) |
| âœ… D2 | Apache Spark Fundamentals âš¡| Learned Spark RDDs, DataFrames, and basic transformations | [LinkedIn Post](https://www.linkedin.com/posts/nimiyakujjwal_databrickswithidc-databricks-apachespark-activity-7416144095024250880-pSjB?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB-AfacBUlcUhAKRaQjr5MosRdgTWZAR600) |
| âœ… D3 | PySpark Transformations | Applied PySpark operations to clean and transform datasets efficiently | [LinkedIn Post](https://www.linkedin.com/posts/nimiyakujjwal_databrickswithidc-databricks-pyspark-activity-7416155804526952448-A4mo?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB-AfacBUlcUhAKRaQjr5MosRdgTWZAR600) |
| ğŸ”„ D4 | Delta Lake Basics | Implemented Delta Lake tables and learned ACID transactions | [LinkedIn Post](******) |
| ğŸ”„ D5 | Delta Lake Advanced | Explored versioning, time travel, and advanced Delta Lake features | [LinkedIn Post](******) |
| ğŸ”„ D6 | Medallion Architecture | Designed Bronzeâ€“Silverâ€“Gold layers for structured data pipelines | [LinkedIn Post](******) |
| ğŸ”„ D7 | Workflows & Jobs | Scheduled and automated Databricks jobs for data pipelines | [LinkedIn Post](******) |
| ğŸ”„ D8 | Unity Catalog | Managed data governance, permissions, and catalog in Databricks | [LinkedIn Post](******) |
| ğŸ”„ D9 | SQL Analytics | Ran SQL queries on Databricks and created interactive reports | [LinkedIn Post](******) |
| ğŸ”„ D10 | Performance Optimization | Applied caching, partitioning, and optimization techniques | [LinkedIn Post](******) |
| ğŸ”„ D11 | Stats & ML Prep | Performed statistical analysis and prepared datasets for ML | [LinkedIn Post](******) |
| ğŸ”„ D12 | MLflow Basics | Tracked machine learning experiments and models using MLflow | [LinkedIn Post](******) |
| ğŸ”„ D13 | Model Comparison | Compared model performances and evaluated metrics | [LinkedIn Post](******) |
| ğŸ”„ D14 | AI & Generative Analytics | Built AI pipelines and experimented with generative AI models | [LinkedIn Post](******) |

---

## ğŸ› ï¸ Tech Stack

For this 14-day challenge, I primarily use the **Databricks free edition**, which offers a free and powerful environment for learning **big data processing** and **AI workflows**.  
The main tools and frameworks included:  

- **Apache Spark & PySpark** â€“ For distributed data processing, transformations, and analytics at scale.  
- **Delta Lake** â€“ To implement reliable, ACID-compliant data lakes with version control and time travel.  
- **SQL** â€“ For querying structured datasets and running analytical operations efficiently.  
- **MLflow** â€“ To track machine learning experiments, register models, and manage workflows.  
- **Python** â€“ As the primary programming language for data manipulation, analytics, and AI tasks.  

This stack allowed me to explore the full **data-to-AI pipeline** in a hands-on environment. 

---

## ğŸŒ± Learning Objective

The objective of this challenge was to develop **industry-ready skills** in Databricks and big data analytics.  
By following a structured approach, I focused on:  

- Gaining **concept clarity** in data engineering and AI workflows  
- Practicing hands-on tasks with **real-world datasets**  
- Understanding **data governance, optimization, and performance tuning**  
- Building **AI-driven analytics** solutions with MLflow and generative AI  

The goal was to combine theoretical knowledge with practical application to become confident in **end-to-end Databricks projects**.

---

## ğŸ¤ Acknowledgements

Thanks to **[Indian Data Club](https://www.linkedin.com/company/indian-data-club/)**, **[Codebasics](https://www.linkedin.com/company/codebasics/)**, and **[Databricks](https://www.linkedin.com/company/databricks/)** for designing this practical and beginner-friendly AI challenge.

---

## ğŸ“¬ Connect With Me

<!-- Typing Animation / ğŸ¤ Connect with me -->
[![Typing SVG](https://readme-typing-svg.herokuapp.com?color=0DAD8D&lines=Letâ€™s+connect+and+collaborate+on+meaningful+projects!;Click+the+buttons+below+to+connect+with+me+directly!)](https://git.io/typing-svg)

<div align="center">
<!-- ğŸ’¼ LinkedIn -->
<a href="https://www.linkedin.com/in/nimiyakujjwal/"><img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" alt="LinkedIn" width="30" height="30"/></a>
<!-- ğŸ“® Gmail -->
<a href="mailto:dev.ujjwalkumar@outlook.com" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/732/732200.png" alt="Email" width="35" height="35"></a>
<!-- ğŸ†” GitHub -->
<a href="https://github.com/nimiyakujjwal" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/733/733553.png" alt="GitHub" width="35" height="35"></a>
<!-- ğŸŒ Website
<a href="https://rajeevtiwari8055.github.io/" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/841/841364.png" alt="Website" width="35" height="35"></a> -->
</div>

<!-- Typing Animation / ğŸ¤ Thanks for Visiting! -->
[![Typing SVG](https://readme-typing-svg.herokuapp.com?color=8A2BE2&lines=ğŸ¤Thank+you+for+visiting+my+profile!)](https://git.io/typing-svg)

<!-- â­ğŸ’« Shower stars if you like my repos -->
<div align="center">
<img src="https://media.giphy.com/media/ObNTw8Uzwy6KQ/giphy.gif" width="30">
<a href="https://github.com/rajeevtiwari8055/rajeevtiwari8055" alt="GitHub Stars" title="Star my repositories">
<img src="https://img.shields.io/badge/Shower_stars_if_you_like_my_repositories-15k?style=for-the-badge&color=f9c513&logo=github&logoColor=black"/>
</a>
</div>

---
